\documentclass[
    a4paper,
    % bibliography=totoc,
    12pt,
    parskip=half,
]{scrarticle}

\usepackage[T1]{fontenc}
\usepackage{mlmodern}
\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage[left=2cm,right=2cm]{geometry}
% \usepackage[backend=biber, style=ieee, maxbibnames=6, minbibnames=1, maxcitenames=3, mincitenames=1]{biblatex}
\usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     citecolor=blue,
% }
\usepackage{subcaption}

% Don't show subsubsection in TOC
\setcounter{tocdepth}{2}

\graphicspath{{images}}

\title{Assignment 2 Report}
\author{Gr√ºbling Alexander (12122371) \\ Preissegger Leo (12122526) \\  Seczer Tobias (12119044) \\ Shanto Md Aminul Islam (12310834) }
\date{\today}

\pagenumbering{gobble}
\begin{document}

\thispagestyle{empty}
\begin{titlepage}
    \makeatletter
    \centering
    {\LARGE \bfseries \sffamily \@title \par}
    \vspace{3\baselineskip}
    {\large \bfseries \sffamily Group 10 \par}
    {\large \@author \par}
    {\large \@date \par}
    \vspace{2\baselineskip}
    \vfill{}
    {\large
    183.630 \\
    Medical Image Processing UE \\
    Semester: S2025 \par}
    \makeatother
\end{titlepage}

\pagenumbering{arabic}

\begin{enumerate}
    \item Data Exploration
    \begin{enumerate}[label=\theenumi.\arabic*.]
        \item Simple \texttt{plt.imshow} together with \texttt{plt.scatter}
        \item Same as 1.1., but the aligned landmarks are centered around \((0, 0)\).
        \item We only need to add the rotation matrix, which is very simple in 2D. In order to apply the rotation matrix,
        we also need to reshape from \([x_1, \ldots , x_N, y_1, \ldots, y_N]\) to \([[x_1, \ldots, x_N], [y_1, \ldots, y_N]]\), then apply
        the rotation matrix, and finally reshape the data back.
        \item We used the PCA module of sklearn.
        I tested the methods with angles and translations of which I knew how the output should look like and it was ok,
        so the final output with our calculated transformations should also be correct.
    \end{enumerate}

    \item Feature Extraction and Edge Detection via Convolutions
    \begin{enumerate}[label=\theenumi.\arabic*.]
        \item Edge Detection via Convolutions
        \begin{enumerate}[label=\alph*)]
            \item Here the two Prewitt kernels are defined as numpy arrays in the variable \texttt{PREWITT\_X} and \texttt{PREWITT\_Y}.
            The values of the kernels are taken from the assignment sheet.
            \item The convolution is done by first padding the image with one zero on each side to avoid problems with the borders.
            Then we use a nested for loop to iterate over the image and apply the convolution by extracting  a \(3 \times 3\) patch of the image and multiplying it with the kernel.
            \item Now three images are randomly sampled from the dataset using \\ \texttt{np.random.choice(len(images), size=3, replace=False)}.
            After the images ares sampled, the convolution is applied to each of them using the function \texttt{conv2d(...)} defined in 2.1.b.
            Finally, the convolved images are plotted.
        \end{enumerate}
        \item Image Feature Computation
    \end{enumerate}

    \item Classification and Feature Selection
    \begin{enumerate}[label=\theenumi.\arabic*.]
        \item Data preparation
        \begin{enumerate}[label=\alph*)]
            \item Splitting the image dataset of 50 images with its corresponding masks where 30 images \texttt{(I\_train)}  and \texttt{masks (y\_train)} use for training. The rest of them are used for testing the Random Forest algorithm. The test images and their masks denote \texttt{I\_test} and \texttt{y\_test}, respectively.

            \item \texttt{RandomForestClassifier} uses from \texttt{sklearn} module. The parameters are \texttt{n\_estimators}, \texttt{max\_features}, \texttt{class\_weight}, \texttt{n\_jobs}, \texttt{oob\_score}, and \texttt{random\_state}. n\_estimators are the number of trees, in the \texttt{Random Forest} algorithm, 100 trees are used for better performance and accuracy.
            To reduce the overfitting and ensuring the diversity of the trees, max\_features parameters are set to \texttt{"sqrt"}. To automatically adjust the weights of the target variable, \texttt{class\_weight} set to "balanced" and utilizing multiple CPU cores of local machine, n\_jobs set to -1. To estimate the generalization accuracy - Out\-of\-bag samples are used, hence \texttt{oob\_score} set to "True". Better oob\_score means the model will perform better with the unseen data. lastly, random\_state ensures the reproducibility of results. \texttt{clf.fit(X\_train\_flat, y\_train\_flat)} trains over the samples and features. 
            \item The Random Forest Algorithm trains on 36,598 pixels with 7 features. Since the out\-of\-bag score calculates validation accuracy of unused samples during training - therefore, 0.939 score means the model will perform well with the random data.
        \end{enumerate}
        \item Random Forest (RF)
        \item U-Net
        \begin{enumerate}[label=\alph*)]
            \item Train U-Net
            \item Interpret U-Net
            \item Now we use the U-Net to predict the segmentation masks for the test images.
            To do so, we first need to create a dataset and a dataloader for the test images, because the U-Net expects the images to be in a specific shape, that the dataloader automatically produces.
            In order to plot the predicted masks, we need to convert them back to numpy arrays and use the \texttt{plot\_prediction\_triplets(...)} function with the input images, the predicted masks, and the ground truth masks.
            \item For the random forest, we achieve a Dice Score of 0.236, a Precision of 0.139, and a Recall of 0.794.
            The U-Net without augmentation achieves a Dice Score of 0.079, a Precision of 0.041, and a Recall of 1.0.
        \end{enumerate}
        \item Shape Particle Filters
    \end{enumerate}
\end{enumerate}

\end{document}
